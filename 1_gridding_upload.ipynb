{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import shutil \n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required function that allow to perform nearest neighbor algorythm\n",
    "\n",
    "def get_nearest(src_points, candidates, k_neighbors=1):\n",
    "    \"\"\"Find nearest neighbors for all source points from a set of candidate points\"\"\"\n",
    "\n",
    "    # Create tree from the candidate points\n",
    "    tree = BallTree(candidates, leaf_size=15, metric='haversine')\n",
    "\n",
    "    # Find closest points and distances\n",
    "    distances, indices = tree.query(src_points, k=k_neighbors)\n",
    "\n",
    "    # Transpose to get distances and indices into arrays\n",
    "    distances = distances.transpose()\n",
    "    indices = indices.transpose()\n",
    "\n",
    "    # Get closest indices and distances (i.e. array at index 0)\n",
    "    # note: for the second closest points, you would take index 1, etc.\n",
    "    closest = indices[0]\n",
    "    closest_dist = distances[0]\n",
    "\n",
    "    # Return indices and distances\n",
    "    return (closest, closest_dist)\n",
    "\n",
    "\n",
    "def nearest_neighbor(left_gdf, right_gdf, return_dist=False):\n",
    "    \"\"\"\n",
    "    For each point in left_gdf, find closest point in right GeoDataFrame and return them.\n",
    "\n",
    "    NOTICE: Assumes that the input Points are in WGS84 projection (lat/lon).\n",
    "    \"\"\"\n",
    "\n",
    "    left_geom_col = left_gdf.geometry.name\n",
    "    right_geom_col = right_gdf.geometry.name\n",
    "\n",
    "    # Ensure that index in right gdf is formed of sequential numbers\n",
    "    right = right_gdf.copy().reset_index(drop=True)\n",
    "\n",
    "    # Parse coordinates from points and insert them into a numpy array as RADIANS\n",
    "    left_radians = np.array(left_gdf[left_geom_col].apply(lambda geom: (geom.x * np.pi / 180, geom.y * np.pi / 180)).to_list())\n",
    "    right_radians = np.array(right[right_geom_col].apply(lambda geom: (geom.x * np.pi / 180, geom.y * np.pi / 180)).to_list())\n",
    "\n",
    "    # Find the nearest points\n",
    "    # -----------------------\n",
    "    # closest ==> index in right_gdf that corresponds to the closest point\n",
    "    # dist ==> distance between the nearest neighbors (in meters)\n",
    "\n",
    "    closest, dist = get_nearest(src_points=left_radians, candidates=right_radians)\n",
    "\n",
    "    # Return points from right GeoDataFrame that are closest to points in left GeoDataFrame\n",
    "    closest_points = right.loc[closest]\n",
    "\n",
    "    # Ensure that the index corresponds the one in left_gdf\n",
    "    closest_points = closest_points.reset_index(drop=True)\n",
    "\n",
    "    # Add distance if requested\n",
    "    if return_dist:\n",
    "        # Convert to meters from radians\n",
    "        earth_radius = 6371000  # meters\n",
    "        closest_points['distance'] = dist * earth_radius\n",
    "\n",
    "    return closest_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example csv is loaded from the reference folder for the creation of the grid, starting from it we define grid's boundaries\n",
    "\n",
    "csv_file = pd.read_csv('C:/Users/maria/OneDrive - Politecnico di Milano/Desktop/GeoinfoProject/Prova/provadati3.csv')\n",
    "lon_min = min(csv_file['lon'])\n",
    "lon_max = max(csv_file['lon'])\n",
    "lat_min = min(csv_file['lat'])\n",
    "lat_max = max(csv_file['lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The grid is created with a loop, denominators used for computing delta_lat and delta_lon are estabilished through QGIS\n",
    "grid = pd.DataFrame(columns = ['id_grid','latitude_grid', 'longitude_grid', 'geometry'])\n",
    "delta_lat = (lat_max - lat_min)/82\n",
    "delta_lon = (lon_max - lon_min)/101\n",
    "i = 0\n",
    "\n",
    "for lon in np.arange(lon_min, lon_max+delta_lon, delta_lon):\n",
    "    for lat in np.arange(lat_min, lat_max + delta_lat, delta_lat):\n",
    "        p = Point(lon, lat)\n",
    "        row = pd.DataFrame([[i, lat, lon, p]], columns = ['id_grid','latitude_grid', 'longitude_grid', 'geometry'])\n",
    "        grid = grid.append(row)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'geometry' column is added in grid (required by nearest neighbor functions)\n",
    "csv_file['geometry'] = csv_file.apply(lambda _: '', axis=1)\n",
    "\n",
    "for i in range(len(csv_file)):\n",
    "    csv_file['geometry'][i] = Point(csv_file['lon'][i], csv_file['lat'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest neighbor algorithm is perfrmed between the grid and the CSV take as example\n",
    "# allow to assign each point of the grid to the closest one in the csv\n",
    "csv_file['id_csv'] = (csv_file.index).astype(int)\n",
    "temp_nn = nearest_neighbor(csv_file, grid, return_dist = False)\n",
    "temp_nn['id_grid'] = temp_nn['id_grid'].astype(int)\n",
    "temp_nn['id_to_add'] = (temp_nn.index).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A merge is performed between the dataframe on the IDs in order to assign the quadkeys\n",
    "temp_nn['id_to_add'] = (temp_nn.index).astype(int)\n",
    "nn_merge = temp_nn.merge(csv_file, left_on = 'id_to_add', right_on = 'id_csv')\n",
    "nn_merge_2 = nn_merge[['id_grid','longitude_grid', 'latitude_grid', 'geometry_x', 'quadkey']]\n",
    "nn_merge_2.to_csv('C:/Users/maria/OneDrive - Politecnico di Milano/Desktop/GeoinfoProject/Controllare/griglia_temp.csv')\n",
    "\n",
    "# We now have obtained a regular grid, most of the points of the grid has a specific quadkey\n",
    "\n",
    "# Some points don't have a quadkey beacuse there are missing data in the csv\n",
    "# missing quadkeys have been added manually through QGIS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV gridding, transformation in NetCDF, metadata realization and upload on ODC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following piece of code check automatically if there are new csv in the reference folder, grid them, transform in\n",
    "# NetCDF, creates the yaml file and upload the in ODC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridData(csvfiles):\n",
    "    # definition of the path to necessary folders and files:\n",
    "    grid_path = 'C:/git/FB_ODC_2021/griglia_csv/GRIGLIA_MILANO.csv'  \n",
    "    path_to_netcdf_folder = 'C:/git/FB_ODC_2021/netcdf_files'\n",
    "    origin = 'C:/git/FB_ODC_2021/empty_yaml.yaml'\n",
    "    # 2) Once having determined the csv to upload we can proceed with the processing of the data\n",
    "    #    First we need to upload the grid (for further details on grid realization see 'grid.py')\n",
    "    grid = pd.read_csv(grid_path)\n",
    "    grid = grid.drop(columns='geometry')\n",
    "    \n",
    "    #    All the csv are loaded as pandas dataframe, then are joined to the grid on quadkey \n",
    "    gridded_csv = []\n",
    "    #all_datetimes = []\n",
    "    for i in range(len(csvfiles)):\n",
    "        temp_df = pd.read_csv(csvfiles[i])\n",
    "        # Not necessary columns are dropped\n",
    "        temp_df = temp_df.drop(columns = ['country','lon', 'lat', 'n_baseline', 'n_difference', 'density_crisis', 'density_baseline', 'percent_change', 'clipped_z_score', 'ds'])\n",
    "        # nan values are set to 0\n",
    "        temp_df['n_crisis'] = temp_df['n_crisis'].replace( '\\\\N', 0)\n",
    "        temp_df['n_crisis'] = temp_df['n_crisis'].astype(float)\n",
    "        # merge on the quadkey\n",
    "        temp_gridded = grid.merge(temp_df, on = 'quadkey', how = 'outer')\n",
    "        temp_gridded = temp_gridded.rename(columns = {'latitude_g':'latitude', 'longitude_g':'longitude'})\n",
    "        temp_gridded['n_crisis'].fillna(0, inplace=True)\n",
    "        temp_gridded.crs = '+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs'\n",
    "        # all the datetimes are stored in a list that will be uset to name the NETCDF files\n",
    "        dt_string = (((csvfiles[i].split('_'))[3]).split('.'))[0]  \n",
    "        dt = datetime.strptime(dt_string, '%Y-%m-%d %H%M')\n",
    "        #all_datetimes.append(datetime)\n",
    "        temp_gridded['date_time'] = dt\n",
    "        #temp_gridded['date_time']  = pd.to_datetime(temp_gridded['date_time'] , format = \"%Y-%m-%dT%H:%M:%S\")\n",
    "        temp_gridded = temp_gridded.rename(columns = {'date_time':'time'})\n",
    "        temp_gridded = temp_gridded[['latitude', 'longitude', 'quadkey', 'n_crisis', 'time']]\n",
    "        temp_gridded = temp_gridded.drop(columns=['quadkey'])\n",
    "        temp_gridded = temp_gridded.sort_values(by = ['latitude', 'longitude'], ascending = [False, True])\n",
    "        temp_gridded = temp_gridded.set_index(['time', 'latitude', 'longitude'])\n",
    "        # all gridded csv are stored in gridded_csv\n",
    "        gridded_csv.append(temp_gridded)\n",
    "        print('ok',i,'gridded')\n",
    "        temp_xarray = temp_gridded.to_xarray()\n",
    "        print('ok',i,'to xarray')\n",
    "        netcdf_path = path_to_netcdf_folder+'/'+dt_string.replace(\" \", \"\") +'.nc'\n",
    "        temp_xarray.to_netcdf(netcdf_path)\n",
    "        print('ok',i,'in netcdf')\n",
    "    \n",
    "        # REALIZATION OF yaml DATASET\n",
    "        datetime_string = dt_string[0:10]+\"T\"+dt_string[11]+dt_string[12]+\":\"+dt_string[13]+dt_string[14]+\":00.000Z\"\n",
    "        PID = list(dt_string)\n",
    "        PID.remove(\"-\")\n",
    "        PID.remove(\"-\")\n",
    "        PID.remove(\" \")\n",
    "        PID = \"\".join(PID)\n",
    "        file1 = open(origin, \"w\")\n",
    "        to_write = \"$schema: https://schemas.opendatacube.org/dataset \\n \\nid: 00000000-0000-0000-0000-\"+PID+\"\\n\\nproduct:\\n  name: MILAN\\n  href: https://dataforgood.fb.com/ \\n  format: NetCDF\\n\\ncrs: epsg:4326\\n\\ngeometry:\\n  type: Polygon\\n  coordinates: [[[ 8.995056152343800, 45.311597470877999], [8.995056152343800, 45.627484179430269], [9.549865722656120, 45.627484179430269], [9.549865722656120, 45.311597470877999], [ 8.995056152343800, 45.311597470877999]]]\\n  crs: epsg::4326\\n\\ngrids:\\n  default:\\n    shape: [102,83] \\n    transform: [0.005493164062498224, 0.0, 8.99230957031255, 0.0, -0.0038522769335641825, 45.62941031789704, 0.0, 0.0, 1.0]\\n\\nlineage: {}\\n\\nmeasurements:\\n  n_crisis:\\n    layer: n_crisis\\n    path: \"+netcdf_path+\"\\n    nodata: -9999\\n\\nproperties:\\n  odc:file_format: NetCDF\\n  datetime: \"+datetime_string\n",
    "        file1.write(to_write)\n",
    "        file1.close()\n",
    "        target = \"C:/git/FB_ODC_2021/cubeenv/dataset/\"+PID+\".yaml\"\n",
    "        shutil.copy(origin, target)\n",
    "        command = \"datacube dataset add \"+target\n",
    "        os.system(command)\n",
    "    # In conclusion the names of the new peocessed csv are now written in loaded_csv.txt \n",
    "    to_write_on_txt = \",\".join(csvfiles)+','\n",
    "    with open(\"C:/git/FB_ODC_2021/cubeenv/loaded_csv.txt\", \"a\") as output:\n",
    "        output.write(to_write_on_txt)\n",
    "    \n",
    "    return 'DONE!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are new csv files in /Coronavirus Disease Prevention/Population Map/milan to upload in OpenDataCube\n",
    "fold_path = 'C:/git/FB_ODC_2021/milan'\n",
    "csvfiles = []\n",
    "for file in glob.glob(fold_path+\"/\"+\"*.csv\"):\n",
    "    # We only consider not-empy files\n",
    "    if os.stat(file).st_size != 0:\n",
    "        csvfiles.append(file)\n",
    "\n",
    "# 'loadedCSV.txt' contains all the names of the already loaded files, already_loaded is a list containing these files names\n",
    "with open(\"C:/git/FB_ODC_2021/loaded_csv.txt\", \"r\") as txt:\n",
    "    already_loaded = (txt.read()).split(',')\n",
    "\n",
    "# in order to check if there are new csv i use a loop that removes from csvfiles list the names of the already loaded csv\n",
    "for name in already_loaded:\n",
    "    if name in csvfiles:\n",
    "        csvfiles.remove(name)  \n",
    "\n",
    "if len(csvfiles) != 0:\n",
    "    print(len(csvfiles), 'new CSVs have been found')\n",
    "    print('Processing of CSVs is started...')\n",
    "    ret = gridData(csvfiles)\n",
    "    if ret == 'DONE!':\n",
    "        print('CSVs have been processed and transformed in NETCDF format, metadata dataset have been created and correctly uploaded in ODC')\n",
    "        \n",
    "    else:\n",
    "        print('Something whent wrong :(')\n",
    "else:\n",
    "    print('No new csv have been found, all csv are already uploaded in ODC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General information about ODC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app = \"FB_ODC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all products contained in ODC database and their characteristics\n",
    "list_of_products = dc.list_products()\n",
    "products = dc.list_products()\n",
    "display_columns = [\"name\",\n",
    "                   \"description\",\n",
    "                   \"platform\",\n",
    "                   \"instrument\",\n",
    "                   \"crs\",\n",
    "                   \"resolution\"]\n",
    "\n",
    "products[display_columns].sort_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
